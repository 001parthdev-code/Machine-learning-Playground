# Next Character Predictor (Bigram + Trigram Backoff Model)

A simple yet powerful **next-character language model** built from scratch using **bigrams, trigrams, and backoff logic**.
This project is part of my ML playground where I build foundational NLP concepts from zero â€” the same principles that scale up to RNNs, LSTMs, and Transformers.

---

## ğŸš€ What This Project Does

Given some text input, the model predicts the **most likely next character** using:

1. **Trigram Model**
   Uses the last *two characters* to predict the next one.

2. **Bigram Model**
   If trigram information is missing, fall back and use the last *one character*.

3. **Backoff Logic**
   A classical NLP approach used before neural networks became dominant.

---

## ğŸ§  Why This Matters

This project is not â€œjust a toy.â€
You are building the **core idea of language modeling**:

> **Use previous context â†’ predict next token.**

This technique was used in:

* Early Google Search autocomplete
* Early speech recognition
* Statistical machine translation
* Pre-neural NLP systems

Your model is the ancestor of GPT-style models, just much smaller.

---

## ğŸ“¦ Dataset (Corpus)

Currently using a small word list:

```python
corpus = ["Hello", "Hi", "Halloween"]
```

You can extend this list with any textâ€”sentences, paragraphs, datasets, etc.

---

## ğŸ§© How It Works

### ğŸ”¹ Bigram Model

Counts how often each character is followed by another:

```
'h' â†’ {'e': 2, 'i': 1}
'e' â†’ {'l': 1}
'l' â†’ {'l': 1, 'o': 1}
```

### ğŸ”¹ Trigram Model

Counts how often pairs of characters `(c1, c2)` are followed by a third character:

```
('h','e') â†’ {'l': 1}
('a','l') â†’ {'l': 2}
('l','l') â†’ {'o': 2}
```

### ğŸ”¹ Backoff Prediction Flow

```
if last two chars exist in trigram â†’ use trigram
else if last char exists in bigram â†’ use bigram
else â†’ return None
```

This mirrors classical NLP language modeling.

---

## ğŸ§ª Example Outputs

Given the corpus, predictions look like:

```
Input: "he" â†’ Output: "l"
Input: "h"  â†’ Output: "e"
Input: "al" â†’ Output: "l"
```

---

## ğŸ“ Project Structure

```
next_character_model/
â”‚
â”œâ”€â”€ main.py   # Bigram + Trigram model logic
â””â”€â”€ run.py    # Runs predictions using imported functions
```

`main.py` does not execute anything automatically â€” it only defines functions.
`run.py` is where you call the model and test outputs.

---

## â–¶ï¸ Usage

Run:

```bash
python run.py
```

---

## ğŸ›  Next Steps (Future Work)

* Add **probability sampling** (instead of always choosing the most common next char)
* Build a **Markov Chain text generator**
* Transition to **RNN** â†’ **LSTM** â†’ **Transformer** models
